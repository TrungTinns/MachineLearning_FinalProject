## Summary
### Individual Research Paper: Evaluation of Optimizer Methods and Continual Learning
This research focuses on understanding and comparing various optimizer methods used in training machine learning models, including SGD, Adam, and RMSProp, evaluated based on convergence speed and the ability to avoid local optima. Additionally, it provides insights into Continual Learning, emphasizing continuous learning capabilities and ensuring model accuracy across testing scenarios.

### Group Machine Learning Problem: Prediction with Diverse Data
This problem addresses the challenge of diverse data, incorporating both numerical and categorical data. Initial analysis includes statistical methods and graphical representations. Basic machine learning models and Ensemble Learning are applied, exploring the performance of Feed Forward Neural Network and Recurrent Neural Network. Measures to prevent overfitting ensure model generalization and stability. Error cases are examined, solutions proposed, and results evaluated to improve model accuracy.

## Table of Contents
1. **Acknowledgement**
2. **Teacher's Confirmation and Assessment Section**
3. **Summary**
4. **Table of Content**
5. **List of Tables, Pictures, Graphs**
6. **Chapter 1: Individual Research**
    - Optimizer methods
      - Definition
![image](https://github.com/TrungTinns/MachineLearning_FinalProject/assets/94519308/7f437a0e-36f0-4381-a548-c469a7eb8614)
![image](https://github.com/TrungTinns/MachineLearning_FinalProject/assets/94519308/a678c8ad-d8a6-47d7-b537-bbbbd197f128)
      - How do Optimizers work?
      - Types of Optimization methods
![image](https://github.com/TrungTinns/MachineLearning_FinalProject/assets/94519308/9dacb302-20df-42da-8cf2-8273c83a34e9)
      - Stochastic Gradient Descent (SGD)
![image](https://github.com/TrungTinns/MachineLearning_FinalProject/assets/94519308/aef97b0c-9c3a-4e94-a9a2-1cdfbf3e217a)
      - Mini-batch Gradient Descent
![image](https://github.com/TrungTinns/MachineLearning_FinalProject/assets/94519308/f7fa7cb9-408c-42f1-8131-8f0ecc4c7a1d)
      - Momentum
![image](https://github.com/TrungTinns/MachineLearning_FinalProject/assets/94519308/a16c8577-20e9-4929-a5a4-1f3b79122527)
      - Adam
      - RMSProp
      - Adagrad
      - Pros and Cons
![image](https://github.com/TrungTinns/MachineLearning_FinalProject/assets/94519308/9d52a46f-8a99-402b-820e-a0c4c9c68740)
![image](https://github.com/TrungTinns/MachineLearning_FinalProject/assets/94519308/691deb5c-dd3a-414c-8289-e344ba5ab90a)
    - Continual Learning
      - Introduction
      - The role of Continual Learning
      - The catastrophic forgetting phenomenon
      - Techniques to help prevent catastrophic forgetting
      - Types of Continual Learning
      - Process of Continual Learning
      - Why should ML models be retrained?
      - Advantages of Continual Learning
      - Limitations of Continual Learning
      - Applications of Continuous Learning
    - Test Production
      - Introduction
      - Steps of Test Production
7. **Chapter 2: Group Problems**
    - Prediction with Diverse Data
8. **Reference**




